不管你使用Kafka是作为队列，还是消息总线，或者是数据存储平台，你总是会通过编写一个生产者向Kafka写数据，一个消费者从Kafka读取数据，或者两者兼而有之的应用。  

打个比方，在一个信用卡交易处理系统中，






### 生产者概述  
一个应用可能有很多种理由需要写消息到Kafka：记录活跃用户用于审计或分析、记录指标、存储日志消息、记录只能家电的信息、应用之间的异步通信、写入数据库前的缓冲、等等。  

那些多样化的案例意味着多样化的需求：每条消息都是必不可少的？可以忍受消息的丢失？可以接受偶然的消息重复？对延迟或者吞吐量有严格的要求？  

在之前我们介绍的信用卡事务处理系统的案例中，绝不可以丢失一条消息或者是重复消息是至关重要的。延迟应该要低，但是延迟到500ms还是可以被接受的，并且吞吐量应该非常高——我们期望一秒内可以处理到100万条消息。  

另一种不同的案例可能是存储网站的单击信息。在那种情况下，一些信息的丢失或者重复是可以被接受的；延迟性可以非常高，只要对用户体验没有任何影响即可。换句话说，我们不关心消息花了多长时间到达Kafka，只要下一个页面在用户点击链接后立刻加载即可。吞吐量则取决于我们对网站期望的活跃度。  

不同的需求条件将会影响你使用生产者API方法，包括写入消息到Kafka和配置Kafka。  

然而生产者API是非常简单的，但是当我们使用生产者发送数据时生产者内部的底层实现还是有一点要讲的。图3-1展示了发送数据到Kafka涉及的主要步骤。  
![image](/Images/Kafka/producer-send-data-main-step.png)  

我们通过创建一个ProducerRecord对象开始往Kafka中生产数据，其中ProducerRecord中必须包含我们想要发送记录和值的主题。我们也可以指定一个key或者一个分区，当然这不是必须的。一旦我们发送ProducerRecord，生产者要做的第一件事就是将key和value的对象序列化为字节数组，所以它们就可以通过网络进行发送。  

接下来，数据会被发送到一个分区器。如果我们在ProducerRecord中指定了一个分区，那么分区器不会做任何事，仅仅是简单的返回我们指定的分区。如果我们没有指定，分区器将会为我们选择一个分区，通常是基于ProducerRecord的key。




### 构造一个生产者  





### 向Kafka发送消息  



#### 同步的发送消息  



#### 异步的发送消息  




### 配置生产者  



#### acks  



#### buffer.memory  


#### compression.type  



#### retries  




#### batch.size  




#### linger.ms  




#### client.id  




#### max.in.flight.requests.per.connection  




#### timeout.ms, request.timeout.ms, and metadata.fetch.timeout.ms  



#### max.block.ms  



#### max.request.size  



#### receive.buffer.bytes和send.buffer.bytes



### 序列化器  



#### 自定义序列化器  



#### 使用Apache Avro进行序列化  




#### 使用Avro Records  




### 分区  






#### 实现一个自定义的分区策略  







### 旧的Producer API
