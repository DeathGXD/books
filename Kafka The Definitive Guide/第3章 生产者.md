不管你使用Kafka是作为队列，还是消息总线，或者是数据存储平台，你总是会通过编写一个生产者向Kafka写数据，一个消费者从Kafka读取数据，或者两者兼而有之的应用。  

打个比方，在一个信用卡交易处理系统中，






### 生产者概述  
一个应用可能有很多种理由需要写消息到Kafka：记录活跃用户用于审计或分析、记录指标、存储日志消息、记录只能家电的信息、应用之间的异步通信、写入数据库前的缓冲、等等。  

那些多样化的案例意味着多样化的需求：每条消息都是必不可少的？可以忍受消息的丢失？可以接受偶然的消息重复？对延迟或者吞吐量有严格的要求？  

在之前我们介绍的信用卡事务处理系统的案例中，绝不可以丢失一条消息或者是重复消息是至关重要的。延迟应该要低，但是延迟到500ms还是可以被接受的，并且吞吐量应该非常高——我们期望一秒内可以处理到100万条消息。  

另一种不同的案例可能是存储网站的单击信息。在那种情况下，一些信息的丢失或者重复是可以被接受的；延迟性可以非常高，只要对用户体验没有任何影响即可。换句话说，我们不关心消息花了多长时间到达Kafka，只要下一个页面在用户点击链接后立刻加载即可。吞吐量则取决于我们对网站期望的活跃度。  

不同的需求条件将会影响你使用生产者API方法，包括写入消息到Kafka和配置Kafka。  

然而生产者API是非常简单的，但是当我们使用生产者发送数据时生产者内部的底层实现还是有一点要讲的。图3-1展示了发送数据到Kafka涉及的主要步骤。  
![image](/Images/Kafka/producer-send-data-main-step.png)  

我们通过创建一个ProducerRecord对象开始往Kafka中生产数据，其中ProducerRecord中必须包含我们想要发送记录和值的主题。我们也可以指定一个key或者一个分区，当然这不是必须的。一旦我们发送ProducerRecord，生产者要做的第一件事就是将key和value的对象序列化为字节数组，所以它们就可以通过网络进行发送。  

接下来，数据会被发送到一个分区器。如果我们在ProducerRecord中指定了一个分区，那么分区器不会做任何事，仅仅是简单的返回我们指定的分区。如果我们没有指定，分区器将会为我们选择一个分区，通常是基于ProducerRecord的key。一旦分区被选定，生产者就知道将记录发送到那个主题和分区。生产者会将记录添加到一个记录批次，那样的话同一个批次的记录都将会发送到一个相同的主题和分区。还有一个特定的线程负责将那些记录批次发送到适当的Kafka的broker。  

当broker接受到消息后，它会返回一个响应。如果消息被成功写入到Kafka，broker会返回一个带有主题、分区和记录在分区内的偏移量的RecordMetadata对象。如果消息写入broker失败，broker将会返回一个错误。当生产者接受到一个错误，它可能会尝试多次发送消息，直到放弃并且返回一个错误。  

### 构造一个生产者  
往Kafka中写消息的第一步是创建一个带有你想要传递给生产者的配置属性的KafkaProducer对象。Kafka生产者有三个必不可少的属性：  
* bootstrap.servers  
    broker的host:port对列表，生产者将使用它建立与Kafka集群的初始连接。这个列表不需要包含所有的broker，因为生产者在建立初始连接后将会获得更多的信息。但还是推荐至少包含至少两个broker，因为万一一个broker挂掉了，生产者仍能够连接到集群。  

* key.serializer  
    一个类名，用于序列化我们生产到Kafka的记录中的key。broker期望字节数组作为消息的key和value。然而，生产者只允许使用任何参数化类型的Java对象作为key和value进行发送。这样造就了非常好的可读性代码，但是同样意味着生产者不得不知道将这些对象转换为字节数组。key.serializer应该设置为实现了org.apache.kafka.common.serialization.Serializer接口的一个类名。生产者会使用这个类将key对象序列化为字节数组。Kafka客户端会打包ByteArraySerializer(不需要做很多)，StringSerializer和IntegerSerializer几个类，因此，如果你使用的通用类型，那就不需要实现你自己的序列化器。设置key.serializer是必须的，即使你仅仅只打算发送value值。  

* value.serializer
    一个类名，用于序列化我们生产到Kafka的记录中的value。这个配置与你设置key.serializer的方式相同，一个用于将消息的key对象序列化为字节数组类名，设置value.serializer，一个用与将消息的value对象序列化为字节数组的类名。  

下面的代码片段展示了仅仅设置必要配置并且所有配置都使用默认值来创建一个新的生产者：  
```java
private Properties kafkaProps = new Properties();//1
kafkaProps.put("bootstrap.servers", "broker1:9092,broker2:9092");

kafkaProps.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");//2
kafkaProps.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");

producer = new KafkaProducer<String, String>(kafkaProps);//3
```  

1. 我们以创建Properties对象开始。
2. 因为我们计划使用字符串作为消息的key和value，所以我们使用内置的StringSerializer。
3. 这里我们通过设置合适的key和value类型来创建一个生产者，并将Properties对象传递进去。  

使用如此简单的一个接口，很明显，大多数对生产者行为的控制是通过设置正确的配置属性来完成的。Apache Kafka官方文档涉及了所有生产者的配置选项，本章后面我们将会复习几个重要的配置参数。  

一旦我们实例化了一个生产者，那就是时候发送消息了。有三个发送消息的基本方法：  
* 即发即弃  
    我们发送一条消息给服务器后，并不真正关心消息是否成功到达。大多数情况下，消息会成功到达，因为Kafka是高可用的并且生产者将会自动重试发送消息。然而，使用这个方法一些消息可能会丢失。  

* 同步发送  
    我们发送了一条消息，send()方法将会返回一个Future对象，然后我们使用get()方法去等待future返回结果，查看send()方法是否成功。  

* 异步发送  
    我们使用一个回调函数调用send()方法，当它从broker那里收到一个响应后将会触发。  

在接下来的例子中，我们将会看到如何使用这三个方法发送消息，并且如何处理可能发生的不同类型的错误。  

由于本章中所有的例子都是单线程的，但是生产者是可以使用多线程去发送消息的。你可能会想要一个生产者一个线程。如果你需要更好的吞吐量，你可以在同一个生产者中增加更多的线程。一旦这种增加吞吐量的方法达到了瓶颈，你可以为应用增加更多的生产者来达到更多的吞吐量。  

### 发送消息  
发送消息最简单的方式如下：  
```java
ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Precision Products", "France");//1
try {
  producer.send(record);//2
} catch (Exception e) {
        e.printStackTrace();//3
}
```  
1. 生产者需要接受一个ProducerRecord对象，因此我们从创建一个开始。ProducerRecord有多个构造函数，我们将会在后面进行讨论。这里我们使用一个，需要发送数据的主题名，一个字符串，一个数据的key和value，本例中也是字符串。其中key和value的类型必须与序列化类型和生产者对象中key和value一致。  
2. 我们使用生产者对象的send()方法去发送ProducerRecord。正如我们在生产者架构图3-1中所看到的，发送的消息会被放置到一个缓冲区，并将在一个独立的线程中发送给broker。send()方法会返回一个带有RecordMetadata的Java的Future对象，但是因为简单的忽略来返回值，所以我们没有方法知道消息是否发送成功。当丢失消息可以被默默接受时，这种发送消息的方法可以被使用。但是在生产环境中不具有代表性。  
3. 虽然我们忽略了当发送消息到broker或者broker自己本身可能发生的错误，但是在发送消息到Kafka之前如果生产者遇到了错误，我们仍然是可以获得一个异常。可能是一个SerializationException，当序列化消息失败的时候，可能是一个BufferExhausedException或者TimeoutException，当缓冲区满了的时候，或者是一个InterruptException，如果发送线程被中断。  

#### 同步的发送消息  
同步发送消息最简单的方式如下：  
```java
ProducerRecord<String, String> record =
            new ProducerRecord<>("CustomerCountry", "Precision Products", "France");
try {
  producer.send(record).get();//1
} catch (Exception e) {
  e.printStackTrace();//2
}
```  
1. 这里我们使用Future.get()方法等待来自于Kafka的回复。使用这个方法，如果记录没有成功发送到Kafka，将会抛出一个异常。如果没有任何错误，我们将得到一个RecordMetadata对象，我们可以从RecordMetadata对象中获取写入Kafka消息的偏移量。  
2. 如果在发送数据到Kafka之前发生了错误，或者当正在发送数据时，如果Kafka broker返回一个不可回溯的异常，或者用完了可用的重试次数，我们也将会遭遇一个异常。在本例中，我们仅仅是打印了我们运行中出现的异常。  

KafkaProducer有两种类型的错误。可回溯错误是哪些可以通过重新发送消息就能解决的。比如，连接错误是可以被解决的，因为连接是可以被重新建立起来的。"no leader"错误是可以被解决的，当为分区选举出一个新的leader时。KafkaProducer可以通过配置自动重试哪些错误，因此，仅仅当重试次数被用尽并且错误还没有被解决的情况下，应用代码才会得到一个可回溯异常。有一些错误通过重试是解决不了的。比如，"消息太大"。在那种情况下，KafkaProducer不会进行重试，并立刻返回一个异常。  

#### 异步的发送消息  
假如我们应用和Kafka集群之间的网络往返时间是10ms。如果我们在每发送一条消息后都等待回复，那么发送100条消息将会耗费我们1s的时间。换句话说，如果我们仅仅只是发送我们所有的消息，并不等待任何回复，那么发送100条消息几乎不会耗费我们任何一点时间。大多数情况下，我们真的不需要回复——Kafka在消息被写入之后，会返回消息的主题、分区和记录的偏移量，通常不需要我们应用完成。再换句话说，我们确实需要知道我们发送消息彻底失败了，因此我们可以抛出一个异常，日志或者错误，或者可以将信息写入到"错误"文件用于之后的分析。  

为了支持异步的发送消息并且仍能够处理错误的情况，生产者支持当发送一条记录时，可以添加一个回调函数。下面是一个我们如何使用回调函数的例子：  
```java
private class DemoProducerCallback implements Callback {

@Override
  public void onCompletion(RecordMetadata recordMetadata, Exception e) {
    if (e != null) {
      e.printStackTrace();
    }
  }
}

ProducerRecord<String, String> record =
                              new ProducerRecord<>("CustomerCountry", "Biomedical Materials", "USA");

producer.send(record, new DemoProducerCallback());
```  
1. 为了使用回调，你需要一个实现了org.apache.kafka.clients.producer.Callback接口的类，它只有一个单一的函数——onCompletion()。  
2. 如果Kafka返回一个错误，onCompletion()将会有一个非空的异常。这里，我们通过打印异常的方式"处理"它，但是生产环境中应该拥有更健壮的错误处理函数。  
3. 记录与之前的相同。  
4. 当我们发送记录时我们会一起传递一个CallBack对象。  

### 配置生产者  
到目前为止，我们只看到了很少的生产者的配置参数——仅仅是必须的bootstrap.servers和序列化器。  

生产者有大量的配置参数；大多数在Apache Kafka文档中已有说明，并且很多参数已经有了合适的默认是，因此，没有理由对每一个单独的参数进行讲解。然而，有些参数对生产者内存的使用，性能和可靠性都有重大的影响，因此，我们在这里将会复习一下。  

#### acks  
acks参数控制了，在生产者可以认为写入成功之前，必须收到记录的分区的多少个副本。这个参数对消息丢失的可能性有多大有着重大的影响。下面是三个acks参数允许的值：  
* 如果acks=0，在假设消息发送成功之前，不会等待来自于broker的回复。这意味着如果发生了一些错误，并且broker没有接受到消息，生产者将不得而知，消息将会丢失。可是，由于生产者不会等待来自于服务器的任何响应，它可以在网络支持的情况下尽可能快的发送消息，因此这个设置可以用于想要到达非常高的吞吐量的情况下。
* 如果acks=1，当leader副本接受到消息后，生产者会收到一个来自于broker的成功响应。如果消息没有被写入到leader(比如，leader崩溃了，新的leader还没有被选举出来)，生产者会收到一个错误响应，并可以重新发送消息，避免了数据潜在丢失的可能。如果leader崩溃了，并且没有该条消息的副本被选举为新的leader(由于不干净的leader选举)，那么消息仍然可能会丢失。在那种情况下，吞吐量依赖于我们发送消息是异步的还是同步的。如果客户端等待来自于服务器的回复(通过调用发送消息时返回的Future对象的get()方法)，它将绝对会显著的增加延迟(至少会增加网络之间的往返时间)。如果客户端使用回调函数，那么延迟将会被隐藏，但是吞吐量将有飞行中的消息量限制(比如，在收到服务器的回复之前，生产者有多少消息将要发送)。  
* 如果acks=all，一旦所有同步的副本都接受到了消息，生产者将会收到来自于broker的成功响应。这是最安全的模式，因为你可以确定有超过一个的broker收到了消息，并且万一有broker崩溃了，消息也会得以存活(更多的信息请参考第5章)。然而，延迟比我们在acks=1时讨论的还要高，因为我们不仅仅会等待一个broker接受到消息。  

#### buffer.memory  
这个配置用于设置生产者使用的用于存放等待被发送到broker的消息的缓冲区的内存大小。如果应用程序发送消息的速度比他们可以被传递到服务器的速度要快，那么生产者很可能用完了缓冲区，并且额外的send()方法调用要么被阻塞要么抛出一个异常，取决于block.on.buffer.full参数(在0.9.0.0版本中被max.block.ms替换，允许阻塞某一段时间然后在抛出一个异常。)  

#### compression.type  
默认情况下，生产者发送的消息是没有被压缩的。这个参数可以设置为snappy、gzip或者lz4，这种情况下，在消息被发送到broker之前，会使用合适的算法压缩数据。Snappy压缩算法由google发明，在使用较低的CPU消耗下获得想当不错的压缩比和压缩性能，因此在比较注重性能和带宽的情况下，snappy压缩会是不错的选择。Gzip压缩通常会使用更多的CPU和时间，但是更好的压缩比，因此在网络带宽限制比较大的情况下，Gzip压缩会是不错的选择。当向Kafka发送消息遇到瓶颈时，通过启用压缩，可以减少网络使用和磁盘存储。  

#### retries  




#### batch.size  




#### linger.ms  




#### client.id  




#### max.in.flight.requests.per.connection  




#### timeout.ms、request.timeout.ms和metadata.fetch.timeout.ms  



#### max.block.ms  



#### max.request.size  



#### receive.buffer.bytes和send.buffer.bytes



### 序列化器  



#### 自定义序列化器  



#### 使用Apache Avro进行序列化  




#### 使用Avro Records  




### 分区  






#### 实现一个自定义的分区策略  







### 旧的Producer API
