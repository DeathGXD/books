本章将会介绍Spark全部的设计原理和它在大数据生态系统中的位置。Spark常常被认为是Apache MapReduce的替代者，以为Spark同样可以和Hadoop一起进行分布式数据处理。随着我们本章的讨论，你会发现Spark的设计原理与那些在MapReduce中的完全不一样。与MapReduce不同的是，Spark不是必须要与Hadoop一起运行——尽管它经常那这样做。Spark从其他已经存在的计算框架借鉴了部分API，设计和支持的格式，特别是DryadLINQ。然而，Spark的内核与大多数传统的系统不同，尤其是它如何处理失败。Spark在内存计算中的惰性计算能力使得它非常的独一无二。Spark的创建者相信它能够成为一个首屈一指的用于快速分布式数据处理的高级别编程语言。  

想要更深入的学习Spark，弄明白Spark的设计原理是非常重要的，并且还应该粗略的了解Spark程序是如何执行的。本章中，我们将会提供一个Spark并行计算模型的概览和对Spark调度引擎和执行引擎一个深入的讲解。本章中的涉及到概念将会贯穿全文。在未来，对于其他Spark用户提到的或者在Spark文档中遇到的，我们希望这些讲解将会提供给你一个更精确的理解。  

### Spark如何适应大数据生态系统  
Apache Spark是一个开源的
